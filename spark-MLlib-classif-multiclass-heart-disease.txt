---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("heart_diseases/processed.cleveland.data").map( x => x.split(","))

rdd.map(x => (x(12),1)).reduceByKey(_+_).take(10)
res0: Array[(String, Int)] = Array((7.0,117), (6.0,18), (3.0,166), (?,2))

rdd.map(x => (x(11),1)).reduceByKey(_+_).take(10)
res1: Array[(String, Int)] = Array((1.0,65), (3.0,20), (?,4), (0.0,176), (2.0,38))

---- needs to replace "?" with most frequent value in the respective field
val rdd1 = rdd.map( x => {
   val arr = x
   arr(12) = if (arr(12) == "?") "3.0" else arr(12)
   arr(11) = if (arr(11) == "?") "0.0" else arr(11)
   arr
 })

rdd1.take(10)
res4: Array[Array[String]] = Array(Array(63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0, 0), Array(67.0, 1.0, 4.0, 160.0, 286.0, 0.0, 2.0, 108.0, 1.0, 1.5, 2.0, 3.0, 3.0, 2), Array(67.0, 1.0, 4.0, 120.0, 229.0, 0.0, 2.0, 129.0, 1.0, 2.6, 2.0, 2.0, 7.0, 1), Array(37.0, 1.0, 3.0, 130.0, 250.0, 0.0, 0.0, 187.0, 0.0, 3.5, 3.0, 0.0, 3.0, 0), Array(41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 2.0, 172.0, 0.0, 1.4, 1.0, 0.0, 3.0, 0))

val rdd2 = rdd1.map( x => x.map( y => y.toDouble ))

rdd2.take(10)
res1: Array[Array[Double]] = Array(Array(63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0, 0.0), Array(67.0, 1.0, 4.0, 160.0, 286.0, 0.0, 2.0, 108.0, 1.0, 1.5, 2.0, 3.0, 3.0, 2.0), Array(67.0, 1.0, 4.0, 120.0, 229.0, 0.0, 2.0, 129.0, 1.0, 2.6, 2.0, 2.0, 7.0, 1.0), Array(37.0, 1.0, 3.0, 130.0, 250.0, 0.0, 0.0, 187.0, 0.0, 3.5, 3.0, 0.0, 3.0, 0.0), Array(41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 2.0, 172.0, 0.0, 1.4, 1.0, 0.0, 3.0, 0.0))

val categ_cp = Map( 1.0 -> 0, 2.0 -> 1, 3.0 -> 2, 4.0 -> 3)

val categ_slope = Map( 1.0 -> 0, 2.0 -> 1, 3.0 -> 2)

val categ_thal = Map( 3.0 -> 0, 6.0 -> 1, 7.0 -> 2)

val rdd2_dt = rdd2.map( x => Array(x(0),x(1),categ_cp(x(2)),x(3),x(4),x(5),x(6),x(7),x(8),x(9),categ_slope(x(10)),x(11),categ_thal(x(12)),x(13)))

rdd2_dt.take(5)
res0: Array[Array[Double]] = Array(Array(63.0, 1.0, 0.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 2.0, 0.0, 1.0), Array(67.0, 1.0, 3.0, 160.0, 286.0, 0.0, 2.0, 108.0, 1.0, 1.5, 1.0, 3.0, 0.0), Array(67.0, 1.0, 3.0, 120.0, 229.0, 0.0, 2.0, 129.0, 1.0, 2.6, 1.0, 2.0, 2.0), Array(37.0, 1.0, 2.0, 130.0, 250.0, 0.0, 0.0, 187.0, 0.0, 3.5, 2.0, 0.0, 0.0), Array(41.0, 0.0, 1.0, 130.0, 204.0, 0.0, 2.0, 172.0, 0.0, 1.4, 0.0, 0.0, 0.0))

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd2.zip(rdd2_dt).map( x => {
   val x1 = x._1
   val l1 = x1(x1.size-1)
   val f1 = x1.slice(0,x1.size-1)
   
   val x2 = x._2
   val l2 = x2(x2.size-1)
   val f2 = x2.slice(0,x2.size-1)
   
   (LabeledPoint(l1,Vectors.dense(f1)),LabeledPoint(l2,Vectors.dense(f2)))
 })
 
val sets = data.randomSplit(Array(0.7,0.3))
val trainSet = sets(0).map( x => x._1)
val testSet = sets(1).map( x => x._1)

trainSet.cache

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res17: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (2.0,1.0), (3.0,1.0), (1.0,0.0), (0.0,2.0), (4.0,2.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 50
validPredicts.count                            // 89
val accuracy = metrics.accuracy   // 0.5617977528089888

metrics.confusionMatrix
res20: org.apache.spark.mllib.linalg.Matrix =
45.0  4.0  0.0  0.0  0.0
9.0   1.0  2.0  3.0  0.0
2.0   2.0  1.0  1.0  3.0
2.0   2.0  1.0  3.0  3.0
1.0   3.0  0.0  1.0  0.0

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = trainSet.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res21: org.apache.spark.mllib.linalg.Vector = [71.0,1.0,4.0,200.0,564.0,1.0,2.0,202.0,1.0,6.2,3.0,3.0,7.0]

matrixSummary.min
res22: org.apache.spark.mllib.linalg.Vector = [29.0,0.0,1.0,94.0,126.0,0.0,0.0,71.0,0.0,0.0,1.0,0.0,3.0]

matrixSummary.mean
res23: org.apache.spark.mllib.linalg.Vector = [54.532710280373834,0.677570093457944,3.1495327102803725,131.911214953271,247.7429906542057,0.1542056074766355,1.0280373831775702,149.43457943925225,0.3317757009345794,1.0869158878504674,1.5700934579439259,0.7056074766355138,4.696261682242988]

matrixSummary.variance
res24: org.apache.spark.mllib.linalg.Vector = [81.28296257294548,0.2194945373173621,0.9352814707560009,329.3019393620288,2940.1073450046083,0.13103856785573254,0.9945153788776275,549.8618972401382,0.22274143302180685,1.4788890351454524,0.3589136062480805,0.912926155061208,3.7429906542056077]

---- Apply standardization to dataset -------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res26: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,1.0), (0.0,0.0), (2.0,0.0), (0.0,1.0), (0.0,0.0), (2.0,1.0), (3.0,1.0), (1.0,0.0), (2.0,2.0), (3.0,2.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 52
validPredicts.count                            // 89
val accuracy = metrics.accuracy   // 0.5842696629213483

metrics.confusionMatrix
res29: org.apache.spark.mllib.linalg.Matrix =
43.0  5.0  1.0  0.0  0.0
7.0   1.0  3.0  4.0  0.0
0.0   4.0  3.0  2.0  0.0
1.0   3.0  0.0  5.0  2.0
0.0   3.0  0.0  2.0  0.0

---- Checked the correlation matrix ---------------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = rdd2.map( x => Vectors.dense(x))

vectors.take(5)
res30: Array[org.apache.spark.mllib.linalg.Vector] = Array([63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0,0.0], [67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0,2.0], [67.0,1.0,4.0,120.0,229.0,0.0,2.0,129.0,1.0,2.6,2.0,2.0,7.0,1.0], [37.0,1.0,3.0,130.0,250.0,0.0,0.0,187.0,0.0,3.5,3.0,0.0,3.0,0.0], [41.0,0.0,2.0,130.0,204.0,0.0,2.0,172.0,0.0,1.4,1.0,0.0,3.0,0.0])

val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})

0.8044 0.9490 0.9840 0.9726 0.3981 0.7137 0.9655 0.5762 0.6835 0.9304 0.6204 0.9208 0.6280  num x age
       0.7907 0.8125 0.7838 0.3427 0.5908 0.8109 0.5392 0.5936 0.7773 0.5175 0.8446 0.6016  num x sex
              0.9471 0.9409 0.3581 0.6894 0.9313 0.6385 0.6826 0.9090 0.6097 0.9143 0.6750  num x cp
                     0.9737 0.4034 0.7135 0.9790 0.5736 0.6804 0.9308 0.5850 0.9239 0.6184  num x trestbps
                            0.3790 0.7158 0.9674 0.5698 0.6606 0.9133 0.5879 0.9071 0.6058  num x chol
                                   0.3175 0.3799 0.2397 0.2613 0.3795 0.3292 0.3792 0.2774  num x fbs
                                          0.6890 0.4528 0.5315 0.6930 0.4852 0.6598 0.5320  num x restecg
                                                 0.5182 0.6215 0.9019 0.5402 0.8991 0.5505  num x thalach
                                                        0.5577 0.6094 0.4288 0.6301 0.6059  num x exang
                                                               0.7775 0.5696 0.7145 0.7036  num x oldpeak
                                                                      0.5734 0.9027 0.6745  num x slope
                                                                             0.6152 0.6892  num x ca
                                                                                    0.7146  num x thal

---- MLlib Decision tree regression -------------------------

val trainSet = sets(0).map( x => x._2)
val testSet = sets(1).map( x => x._2)

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

  -- 1. #3  (age)
  -- 2. #4  (sex) -> 2
  -- 3. #9  (cp) -> 4
  -- 4. #10 (trestbps)
  -- 5. #12 (chol)
  -- 6. #16 (fbs) -> 2
  -- 7. #19 (restecg) -> 3
  -- 8. #32 (thalach)
  -- 9. #38 (exang) -> 2
  -- 10. #40 (oldpeak)
  -- 11. #41 (slope) -> 3
  -- 12. #44 (ca) -> 4
  -- 13. #51 (thal) -> 3
  
val categoricalFeaturesInfo = Map[Int, Int]( 1 -> 2, 2 -> 4, 5 -> 2, 6 -> 3, 8 -> 2, 10 -> 3, 11 -> 4, 12 -> 3)

val model = DecisionTree.trainClassifier(trainSet, 5, categoricalFeaturesInfo, "gini", 30, 32)

model.toDebugString
res11: String =
"DecisionTreeModel classifier of depth 12 with 141 nodes
  If (feature 2 in {0.0,1.0,2.0})
   If (feature 9 <= 1.95)
    If (feature 3 <= 162.0)
     If (feature 11 in {1.0,0.0})
      If (feature 4 <= 223.5)
       Predict: 0.0
      Else (feature 4 > 223.5)
       If (feature 1 in {0.0})
        If (feature 4 <= 239.5)
         If (feature 0 <= 45.5)
          Predict: 0.0
         Else (feature 0 > 45.5)
          Predict: 1.0
        Else (feature 4 > 239.5)
         Predict: 0.0
       Else (feature 1 not in {0.0})
        If (feature 0 <= 56.5)
         If (feature 4 <= 243.5)
          If (feature 10 in {0.0})
           If (feature 4 <= 239.5)
            Predict: 0.0
           Else (feature 4 > 239.5)
            Predict: 1.0
          Else (feature 10 not in {...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res12: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (2.0,1.0), (0.0,2.0), (0.0,0.0), (3.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,4.0), (0.0,0.0), (0.0,0.0), (0.0,4.0), (0.0,0.0), (0.0,0.0), (3.0,1.0), (3.0,2.0), (4.0,1.0), (3.0,2.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 48
validPredicts.count                            // 99
val accuracy = metrics.accuracy   // 0.48484848484848486

metrics.confusionMatrix
res15: org.apache.spark.mllib.linalg.Matrix =
39.0  9.0  4.0  4.0  0.0
7.0   5.0  1.0  4.0  1.0
3.0   2.0  2.0  3.0  0.0
2.0   2.0  3.0  1.0  2.0
2.0   0.0  2.0  0.0  1.0

