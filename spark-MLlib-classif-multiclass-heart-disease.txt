---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("heart_diseases/processed.cleveland.data").map( x => x.split(","))

rdd.map(x => (x(12),1)).reduceByKey(_+_).take(10)
res0: Array[(String, Int)] = Array((7.0,117), (6.0,18), (3.0,166), (?,2))

rdd.map(x => (x(11),1)).reduceByKey(_+_).take(10)
res1: Array[(String, Int)] = Array((1.0,65), (3.0,20), (?,4), (0.0,176), (2.0,38))

---- needs to replace "?" with most frequent value in the respective field
val rdd1 = rdd.map( x => {
   val arr = x
   arr(12) = if (arr(12) == "?") "3.0" else arr(12)
   arr(11) = if (arr(11) == "?") "0.0" else arr(11)
   arr
 })

rdd1.take(10)
res4: Array[Array[String]] = Array(Array(63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0, 0), Array(67.0, 1.0, 4.0, 160.0, 286.0, 0.0, 2.0, 108.0, 1.0, 1.5, 2.0, 3.0, 3.0, 2), Array(67.0, 1.0, 4.0, 120.0, 229.0, 0.0, 2.0, 129.0, 1.0, 2.6, 2.0, 2.0, 7.0, 1), Array(37.0, 1.0, 3.0, 130.0, 250.0, 0.0, 0.0, 187.0, 0.0, 3.5, 3.0, 0.0, 3.0, 0), Array(41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 2.0, 172.0, 0.0, 1.4, 1.0, 0.0, 3.0, 0))

val rdd2 = rdd1.map( x => x.map( y => y.toDouble ))

rdd2.take(10)
res5: Array[Array[Double]] = Array(Array(63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0, 0.0), Array(67.0, 1.0, 4.0, 160.0, 286.0, 0.0, 2.0, 108.0, 1.0, 1.5, 2.0, 3.0, 3.0, 2.0), Array(67.0, 1.0, 4.0, 120.0, 229.0, 0.0, 2.0, 129.0, 1.0, 2.6, 2.0, 2.0, 7.0, 1.0), Array(37.0, 1.0, 3.0, 130.0, 250.0, 0.0, 0.0, 187.0, 0.0, 3.5, 3.0, 0.0, 3.0, 0.0), Array(41.0, 0.0, 2.0, 130.0, 204.0, 0.0, 2.0, 172.0, 0.0, 1.4, 1.0, 0.0, 3.0, 0.0))

---- load data to LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd2.map( x => {
  val arr_size = x.size - 1
  val l = x(arr_size)
  val f = x.slice(0,arr_size)
  LabeledPoint(l,Vectors.dense(f))
})

data.take(10)
res20: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0]), (2.0,[67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0]), (1.0,[67.0,1.0,4.0,120.0,229.0,0.0,2.0,129.0,1.0,2.6,2.0,2.0,7.0]), (0.0,[37.0,1.0,3.0,130.0,250.0,0.0,0.0,187.0,0.0,3.5,3.0,0.0,3.0]), (0.0,[41.0,0.0,2.0,130.0,204.0,0.0,2.0,172.0,0.0,1.4,1.0,0.0,3.0]), (0.0,[56.0,1.0,2.0,120.0,236.0,0.0,0.0,178.0,0.0,0.8,1.0,0.0,3.0]), (3.0,[62.0,0.0,4.0,140.0,268.0,0.0,2.0,160.0,0.0,3.6,3.0,2.0,3.0]), (0.0,[57.0,0.0,4.0,120.0,354.0,0.0,0.0,163.0,1.0,0.6,1.0,0.0,3.0]), (2.0,[63.0,1.0,4.0,130.0,254.0,0.0,2.0,147.0,0.0,1.4,2.0,1.0,7.0]), (1.0,[53.0,1.0,4.0,140.0,203.0,1.0,2.0,155.0,1.0,3.1,3.0,0.0,7.0]))

data.map{ case LabeledPoint(x,y) => x }.distinct.take(10)
res7: Array[Double] = Array(2.0, 0.0, 3.0, 1.0, 4.0)

val sets = data.randomSplit(Array(0.7,0.3))
val trainSet = sets(0)
val testSet = sets(1)

---- MLlib logistic regression --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainSet, numIterations)
ERROR DataValidators: Classification labels should be 0 or 1. Found 60 invalid labels


---- MLlib SVM regression --------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainSet, numIterations)
ERROR DataValidators: Classification labels should be 0 or 1. Found 60 invalid labels

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res21: Array[(Double, Double)] = Array((0.0,0.0), (2.0,2.0), (3.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,3.0), (0.0,0.0), (3.0,4.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,1.0), (0.0,0.0), (3.0,3.0), (1.0,3.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 55
validPredicts.count                            // 91
val accuracy = metrics.accuracy   // 0.5670103092783505

metrics.confusionMatrix
res37: org.apache.spark.mllib.linalg.Matrix =
47.0  4.0  0.0  0.0  0.0
11.0  1.0  2.0  2.0  0.0
0.0   6.0  5.0  1.0  0.0
2.0   3.0  4.0  2.0  2.0
0.0   1.0  1.0  3.0  0.0

---- Analyzing statistics for standardization ---------------------

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = data.map{ case LabeledPoint(x,y) => y }
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()

matrixSummary.max
res24: org.apache.spark.mllib.linalg.Vector = [77.0,1.0,4.0,200.0,564.0,1.0,2.0,202.0,1.0,6.2,3.0,3.0,7.0]

matrixSummary.min
res25: org.apache.spark.mllib.linalg.Vector = [29.0,0.0,1.0,94.0,126.0,0.0,0.0,71.0,0.0,0.0,1.0,0.0,3.0]

matrixSummary.mean
res13: org.apache.spark.mllib.linalg.Vector = res26: org.apache.spark.mllib.linalg.Vector = [54.43894389438945,0.6798679867986799,3.158415841584159,131.68976897689765,246.69306930693077,0.1485148514851485,0.9900990099009899,149.6072607260726,0.32673267326732675,1.0396039603960399,1.6006600660065995,0.6633663366336632,4.722772277227725]

matrixSummary.variance
res27: org.apache.spark.mllib.linalg.Vector = [81.69741874849747,0.2183681944353376,0.9218411907415909,309.75112014512706,2680.8491902170376,0.12687692610320636,0.989967870959281,523.265774921863,0.22070683889581014,1.3480952068716803,0.3797346622079425,0.8730575044259389,3.757327388367976]


---- Apply standardization to dataset -------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, false).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))

---- MLlib Multiclass logistic regression --------------

import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
val numIterations = 100
val model = new LogisticRegressionWithLBFGS().setNumClasses(5).run(trainScaled)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res29: Array[(Double, Double)] = Array((0.0,0.0), (2.0,2.0), (3.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (4.0,3.0), (0.0,0.0), (3.0,4.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (2.0,2.0), (0.0,1.0), (0.0,0.0), (3.0,3.0), (2.0,3.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 52
validPredicts.count                            // 91
val accuracy = metrics.accuracy   // 0.5360824742268041

metrics.confusionMatrix
res36: org.apache.spark.mllib.linalg.Matrix =
43.0  6.0  0.0  1.0  1.0
10.0  1.0  3.0  2.0  0.0
0.0   5.0  5.0  2.0  0.0
1.0   1.0  6.0  3.0  2.0
0.0   1.0  2.0  2.0  0.0

---- Checked the correlation matrix ---------------------

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

val vectors = rdd2.map( x => Vectors.dense(x))

vectors.take(5)
res33: Array[org.apache.spark.mllib.linalg.Vector] = Array([63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0,0.0], [67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0,2.0], [67.0,1.0,4.0,120.0,229.0,0.0,2.0,129.0,1.0,2.6,2.0,2.0,7.0,1.0], [37.0,1.0,3.0,130.0,250.0,0.0,0.0,187.0,0.0,3.5,3.0,0.0,3.0,0.0], [41.0,0.0,2.0,130.0,204.0,0.0,2.0,172.0,0.0,1.4,1.0,0.0,3.0,0.0])

val matrix = new RowMatrix(vectors)

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})

0.8044 0.9490 0.9840 0.9726 0.3981 0.7137 0.9655 0.5762 0.6835 0.9304 0.6204 0.9208 0.6280  num x age
       0.7907 0.8125 0.7838 0.3427 0.5908 0.8109 0.5392 0.5936 0.7773 0.5175 0.8446 0.6016  num x sex
              0.9471 0.9409 0.3581 0.6894 0.9313 0.6385 0.6826 0.9090 0.6097 0.9143 0.6750  num x cp
                     0.9737 0.4034 0.7135 0.9790 0.5736 0.6804 0.9308 0.5850 0.9239 0.6184  num x trestbps
                            0.3790 0.7158 0.9674 0.5698 0.6606 0.9133 0.5879 0.9071 0.6058  num x chol
                                   0.3175 0.3799 0.2397 0.2613 0.3795 0.3292 0.3792 0.2774  num x fbs
                                          0.6890 0.4528 0.5315 0.6930 0.4852 0.6598 0.5320  num x restecg
                                                 0.5182 0.6215 0.9019 0.5402 0.8991 0.5505  num x thalach
                                                        0.5577 0.6094 0.4288 0.6301 0.6059  num x exang
                                                               0.7775 0.5696 0.7145 0.7036  num x oldpeak
                                                                      0.5734 0.9027 0.6745  num x slope
                                                                             0.6152 0.6892  num x ca
                                                                                    0.7146  num x thal

---- MLlib Decision tree regression -------------------------

val categ_cp = rdd2.map( x => x(2)).distinct.zipWithIndex.collect.toMap
categ_cp: scala.collection.immutable.Map[Double,Long] = Map(2.0 -> 0, 3.0 -> 1, 1.0 -> 2, 4.0 -> 3)

val categ_slope = rdd2.map( x => x(10)).distinct.zipWithIndex.collect.toMap
categ_slope: scala.collection.immutable.Map[Double,Long] = Map(2.0 -> 0, 3.0 -> 1, 1.0 -> 2)

val categ_thal = rdd2.map( x => x(12)).distinct.zipWithIndex.collect.toMap
categ_thal: scala.collection.immutable.Map[Double,Long] = Map(6.0 -> 0, 7.0 -> 1, 3.0 -> 2)

val trainSet_dt = trainSet.map{ case LabeledPoint(x,y) => {
  val l = x
  val f = Vectors.dense(y(0),y(1),categ_cp(y(2)),y(3),y(4),y(5),y(6),y(7),y(8),y(9),categ_slope(y(10)),y(11),categ_thal(y(12)))
  LabeledPoint(l,f)
}}

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 1 -> 2, 2 -> 4, 5 -> 2, 6 -> 3, 8 -> 2, 10 -> 3, 11 -> 4, 12 -> 3)

val model = DecisionTree.trainClassifier(trainSet_dt, 5, categoricalFeaturesInfo, "gini", 30, 32)

val testSet_dt = testSet.map{ case LabeledPoint(x,y) => {
  val l = x
  val f = Vectors.dense(y(0),y(1),categ_cp(y(2)),y(3),y(4),y(5),y(6),y(7),y(8),y(9),categ_slope(y(10)),y(11),categ_thal(y(12)))
  LabeledPoint(l,f)
}}

val validPredicts = testSet_dt.map(x => (model.predict(x.features),x.label))

scala> validPredicts.take(20)
res62: Array[(Double, Double)] = Array((0.0,0.0), (3.0,2.0), (4.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (3.0,3.0), (0.0,0.0), (2.0,4.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (3.0,0.0), (1.0,2.0), (0.0,1.0), (0.0,0.0), (3.0,3.0), (2.0,3.0), (3.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 50
validPredicts.count                            // 97
val accuracy = metrics.accuracy   // 0.5154639175257731

metrics.confusionMatrix
res65: org.apache.spark.mllib.linalg.Matrix =
38.0  10.0  0.0  3.0  0.0
7.0   5.0   2.0  1.0  1.0
2.0   5.0   2.0  2.0  1.0
1.0   3.0   4.0  5.0  0.0
0.0   2.0   2.0  1.0  0.0

